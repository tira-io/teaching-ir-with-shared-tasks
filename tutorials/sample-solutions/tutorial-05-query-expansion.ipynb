{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IR Lab WiSe 2023: Query Expansion\n",
    "\n",
    "This tutorial shows how to configure and use query expansion with Bo1 in PyTerrier.\n",
    "\n",
    "**Attention:** The scenario below is cherry-picked to explain the concept with a minimal example. There are more query expansion approaches available in PyTerrier, please do not hesitate to look into them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation: Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is only needed in Google Colab, in a dev container, everything should be installed already\n",
    "!pip3 install python-terrier chatnoir-api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Scenario\n",
    "\n",
    "We want to build a search engine for pets.\n",
    "\n",
    "Our search engine has the following five documents:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    {'docno': 'd1', 'text': 'The Golden Retriever is a Scottish breed of medium size.'},\n",
    "    {'docno': 'd2', 'text': 'Intelligent types of dogs are: (1) Border Collies, (2) Poodles, and (3) German Shepherds.'},\n",
    "    {'docno': 'd3', 'text': 'Poodles are a highly intelligent, energetic, and sociable.'},\n",
    "    {'docno': 'd4', 'text': 'The European Shorthair is medium-sized to large cat with a well-muscled chest.'},\n",
    "    {'docno': 'd5', 'text': 'The domestic canary is a small songbird.'}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create an index containing our five documents and use BM25 as retrieval model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyterrier as pt\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 0)\n",
    "\n",
    "if not pt.started():\n",
    "    pt.init(boot_packages=[\"com.github.terrierteam:terrier-prf:-SNAPSHOT\"])\n",
    "\n",
    "indexer = pt.IterDictIndexer(\"/tmp/index\", overwrite=True, blocks=True, meta={'docno': 100, 'text': 20480}, )\n",
    "index_ref = indexer.index(documents)\n",
    "index = pt.IndexFactory.of(index_ref)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25 = pt.BatchRetrieve(index, wmodel=\"BM25\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Problem\n",
    "\n",
    "During our first tests of our search engine, we observed that we have a vocabulary mismatch problem: For the query `dog`, only the document `d2` is albeit the documents `d1` and `d3` are also about dogs (as Golden Retrievers and Poodles are instances of dogs).\n",
    "\n",
    "Lets look into the problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>docid</th>\n",
       "      <th>docno</th>\n",
       "      <th>rank</th>\n",
       "      <th>score</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>d2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.284654</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  qid  docid docno  rank     score query\n",
       "0  1   1      d2    0     1.284654  dog "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# searching for dog returns only document d2 as d1 and d3 have no occurence of the term dog\n",
    "bm25.search(\"dog\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Solution\n",
    "\n",
    "One potential solution for our vocabulary mismatch problem is [query expansion](https://pyterrier.readthedocs.io/en/latest/rewrite.html).\n",
    "\n",
    "Before we start to implement our query expansion approach, we create a small Cranfield-Style collection to measure if our new approach improves the retrieval effectiveness:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The information needs that we want to test\n",
    "import pandas as pd\n",
    "\n",
    "topics = pd.DataFrame([\n",
    "    {'qid': '1', 'query': 'dog'},\n",
    "])\n",
    "\n",
    "qrels = pd.DataFrame([\n",
    "    {'qid': '1', 'docno': 'd1', 'relevance': 1}, #d1 is about an specific dog\n",
    "    {'qid': '1', 'docno': 'd2', 'relevance': 1}, #d1 is about multiple types of dogs\n",
    "    {'qid': '1', 'docno': 'd3', 'relevance': 1}, #d1 is about an specific dog\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>ndcg_cut_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BR(BM25)</td>\n",
       "      <td>0.469279</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       name  ndcg_cut_5\n",
       "0  BR(BM25)  0.469279  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt.Experiment([bm25], topics, qrels, eval_metrics=['ndcg_cut_5'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we can measure the effectiveness, lets try to improve the effectiveness with query expansion.\n",
    "\n",
    "\n",
    "Query expansion algorithms like Bo1 use relevance feedback to expand the query with terms that are prominent in the relevance feedback. In most cases, the relevance feedback is implicit, e.g., we assume that the top results of BM25 are pseudo-relevant.\n",
    "\n",
    "Let us implement the following pipeline:\n",
    "\n",
    "- We use BM25 as pseudo relevance feedback\n",
    "- We use the top-ranked documents of BM25 to expand the query with Bo1 (for our `dog` query, we only have one document for relevance feedback as seen above)\n",
    "- We retrieve the final results using the expanded query against BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "bo1_expansion = bm25 >> pt.rewrite.Bo1QueryExpansion(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>query_0</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>dog</td>\n",
       "      <td>applypipeline:off dog^2.000000000 colli^1.000000000 3^1.000000000 border^1.000000000 shepherd^1.000000000 1^1.000000000 type^1.000000000 german^1.000000000 2^1.000000000 poodl^0.805050646</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  qid query_0  \\\n",
       "0  1   dog      \n",
       "\n",
       "                                                                                                                                                                                         query  \n",
       "0  applypipeline:off dog^2.000000000 colli^1.000000000 3^1.000000000 border^1.000000000 shepherd^1.000000000 1^1.000000000 type^1.000000000 german^1.000000000 2^1.000000000 poodl^0.805050646  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bo1_expansion(topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Bo1 query expansion adds additional terms (already stemmed) like `colli`, `shepherd`, etc. to the query, but still puts the highest weight to the term `dog`.\n",
    "\n",
    "We now can build our final pipeline and use this expanded query for retrieval against BM25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_bo1 = bo1_expansion >> bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>docid</th>\n",
       "      <th>docno</th>\n",
       "      <th>rank</th>\n",
       "      <th>score</th>\n",
       "      <th>query_0</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>d2</td>\n",
       "      <td>0</td>\n",
       "      <td>6.895176</td>\n",
       "      <td>dog</td>\n",
       "      <td>applypipeline:off dog^2.000000000 colli^1.000000000 3^1.000000000 border^1.000000000 shepherd^1.000000000 1^1.000000000 type^1.000000000 german^1.000000000 2^1.000000000 poodl^0.805050646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>d3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.236991</td>\n",
       "      <td>dog</td>\n",
       "      <td>applypipeline:off dog^2.000000000 colli^1.000000000 3^1.000000000 border^1.000000000 shepherd^1.000000000 1^1.000000000 type^1.000000000 german^1.000000000 2^1.000000000 poodl^0.805050646</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  qid  docid docno  rank     score query_0  \\\n",
       "0  1   1      d2    0     6.895176  dog      \n",
       "1  1   2      d3    1     0.236991  dog      \n",
       "\n",
       "                                                                                                                                                                                         query  \n",
       "0  applypipeline:off dog^2.000000000 colli^1.000000000 3^1.000000000 border^1.000000000 shepherd^1.000000000 1^1.000000000 type^1.000000000 german^1.000000000 2^1.000000000 poodl^0.805050646  \n",
       "1  applypipeline:off dog^2.000000000 colli^1.000000000 3^1.000000000 border^1.000000000 shepherd^1.000000000 1^1.000000000 type^1.000000000 german^1.000000000 2^1.000000000 poodl^0.805050646  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25_bo1.search('dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>ndcg_cut_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BM25 &gt;&gt; Bo1 &gt;&gt; BM25</td>\n",
       "      <td>0.765361</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  name  ndcg_cut_5\n",
       "0  BM25 >> Bo1 >> BM25  0.765361  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt.Experiment([bm25_bo1], topics, qrels, eval_metrics=['ndcg_cut_5'], names=['BM25 >> Bo1 >> BM25'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "Our query expansion improved the nDCG@5 quite substantially from 0.47 to 0.77.\n",
    "\n",
    "To summarize everything, please answer the following questions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1:\n",
    "\n",
    "Is query expansion a precision-oriented or a recall-oriented technique?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Solution\n",
    "\n",
    "Query expansion is recall-oriented because more documents are retrieved for terms that would not match the original query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2:\n",
    "\n",
    "Please describe a potential problem that can be caused by query expansion? How would this problem influence precision respectively recall?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Solution\n",
    "\n",
    "- Problems for precision, not for recall (as recall can only get larger)\n",
    "- Query drift\n",
    "- The query expansion approaches that we used ignore diversification, e.g., query `jaguar`: car vs. animal\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3:\n",
    "\n",
    "Our query expansion approach above was corpus-dependent. Do you think, that corpus-independent approaches (e.g., using ChatGPT without context, using Wordnet, etc.) would amplify or reduce the potential problem that you pointed out in question 2? How would they compare in terms of precision respectively recall?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Solution\n",
    "\n",
    "- Corpus-independent query expansion would be at higher risk on query drift, because they can drift to aspects not represented in the corpus for which stemming-mistakes are retrieved.\n",
    "- Corpus-independent approaches would put more emphasis on recall at cost of precision"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
