{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# IR Lab Tutorial: Tokenization\n",
        "\n",
        "This tutorial shows how to use a tokenizer.\n",
        "Tokenization turns a text into a token sequence (token $\\approx$ word).\n",
        "\n",
        "**Attention:** The scenario below is cherry-picked to explain the concept of Tokenization with a minimal example.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preparation: Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# This is needed in both Google Colab and in a dev container\n",
        "!pip3 install -q nltk transformers sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUSMZi7yelLo",
        "outputId": "c327a108-52ad-4eeb-e6d4-3e6a0027bb49"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Our Scenario\n",
        "\n",
        "We want to build a search engine and need to tokenize text into tokens in the text analysis and query analysis step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qq7xBaUCeVBx"
      },
      "outputs": [],
      "source": [
        "sentence_en = \"At eight o'clock on Thursday morning Arthur didn't feel very good.\"\n",
        "sentence_de = \"Donnerstag morgens um acht Uhr fühlte sich Arthur nicht so gut.\"\n",
        "spaces = \">   <\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JR0Y2KolFke"
      },
      "source": [
        "# Word Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Naive Approach\n",
        "\n",
        "A naive approach to word tokenization is to split the text at whitespace characters.\n",
        "\n",
        "However, this approach does not handle punctuation properly. \n",
        "For example, the sentence `\"Hello, world!\"` would be tokenized into `[\"Hello,\", \"world!\"]`, which includes punctuation marks attached to the words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['At', 'eight', \"o'clock\", 'on', 'Thursday', 'morning', 'Arthur', \"didn't\", 'feel', 'very', 'good.']\n",
            "['Donnerstag', 'morgens', 'um', 'acht', 'Uhr', 'fühlte', 'sich', 'Arthur', 'nicht', 'so', 'gut.']\n",
            "['>', '<']\n"
          ]
        }
      ],
      "source": [
        "print(sentence_en.split())\n",
        "print(sentence_de.split())\n",
        "print(spaces.split())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## NLTK's Word Tokenizer\n",
        "\n",
        "A more advanced approach is to use [NLTK's word tokenizer](https://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize) which separates and identifies punctuation and special characters.\n",
        "The tokenizer `word_tokenize`is a wrapper for the `Punkt` sentence tokenizer and the `Treebank` word tokenizer.\n",
        "\n",
        "```word_tokenize() --> Punkt Sentence Tokenizer --> Treebank Word Tokenizer```\n",
        "\n",
        "- The [`Punkt`](https://www.nltk.org/_modules/nltk/tokenize/punkt.html) [sentence tokenizer](https://www.nltk.org/_modules/nltk/tokenize.html#sent_tokenize) uses a unsupervised algorithm to build a model for abbreviation words, collocations, and words that start sentences. It must be trained on a large collection from plaintext in the target language before it can be used. The NLTK data package includes a pre-trained Punkt tokenizer for English. [[Reference](https://www.nltk.org/api/nltk.tokenize.punkt.html)]\n",
        "- The [`Treebank`](https://www.nltk.org/_modules/nltk/tokenize/treebank.html) word tokenizer uses regular expressions to tokenize text as done in the [Penn Treebank](https://aclanthology.org/J93-2004.pdf). [[Reference](https://www.nltk.org/api/nltk.tokenize.treebank.html)]\n",
        "\n",
        "- Finde more sample usages of NLTK's word tokenizers in the [documentation](https://www.nltk.org/_modules/nltk/tokenize/punkt.html).\n",
        "We have downloaded `nltk.download('punkt_tab')` for this purpose."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LS5TlVdveH2I",
        "outputId": "eec5704e-d2a3-4586-b299-727f403c1672"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['At', 'eight', \"o'clock\", 'on', 'Thursday', 'morning', 'Arthur', 'did', \"n't\", 'feel', 'very', 'good', '.']\n",
            "['Donnerstag', 'morgens', 'um', 'acht', 'Uhr', 'fühlte', 'sich', 'Arthur', 'nicht', 'so', 'gut', '.']\n",
            "['>', '<']\n"
          ]
        }
      ],
      "source": [
        "print(nltk.word_tokenize(sentence_en))\n",
        "print(nltk.word_tokenize(sentence_de))\n",
        "print(nltk.word_tokenize(spaces))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdMhTKWZgizN"
      },
      "source": [
        "## Byte Pair Encoding (BPE)\n",
        "\n",
        "BPE [Sennrich et al.](https://aclanthology.org/P16-1162/) is a **subword tokenization algorithm** used by a lot of Transformer models like GPT, GPT-2, RoBERTa, BART, and DeBERTa [[Reference](https://huggingface.co/learn/llm-course/chapter6/5)].\n",
        "It allows tokenizers to handle out-of-vocabulary words by splitting words into smaller subword units."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following example illustrates how training BPE works [[Reference](https://huggingface.co/learn/llm-course/chapter6/5)]:\n",
        "\n",
        "Assume our corpus contains the words: `hug`, `pug`, `pun`, `bun`, `hugs` with the following frequencies:\n",
        "`(\"hug\", 10), (\"pug\", 5), (\"pun\", 12), (\"bun\", 4), (\"hugs\", 5)`.\n",
        "\n",
        "1. Firstly, the vocabulary is initialized with the characters from the corpus. Each word is represented as a sequence of characters, plus a special end-of-word symbol `</w>`.\n",
        "\n",
        "   ```\n",
        "   base vocabulary: [\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\"]\n",
        "   ```\n",
        "\n",
        "2. Count all symbol pairs and find the most frequent pair of consecutive symbols across the corpus.\n",
        "\n",
        "   ```\n",
        "   Pairs in corpus: (\"h\" \"u\", 15), (\"u\" \"g\", 20), (\"p\" \"u\", 5), (\"u\" \"n\", 16), (\"b\" \"u\", 4), (\"g\" \"s\", 5)\n",
        "   Most frequent pair: (\"u\", \"g\")\n",
        "   ```\n",
        "\n",
        "3. Merge the most frequent pair and replace all occurrences of the most frequent pair with a new symbol. [[Reference](https://arxiv.org/abs/1508.07909)]  \n",
        "\n",
        "    The first merge rule learned by the tokenizer is `(\"u\", \"g\") -> \"ug\"`, which means that `\"ug\"` will be added to the vocabulary, and the pair should be merged in all the words of the corpus.\n",
        "    At the end of this stage, the vocabulary and corpus look like this:\n",
        "\n",
        "   ```\n",
        "   Vocabulary: [\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\", \"ug\"]\n",
        "   Corpus: (\"h\" \"ug\", 10), (\"p\" \"ug\", 5), (\"p\" \"u\" \"n\", 12), (\"b\" \"u\" \"n\", 4), (\"h\" \"ug\" \"s\", 5)\n",
        "   ```\n",
        "\n",
        "4. Continue merging the most frequent pairs.\n",
        "\n",
        "   ```\n",
        "   Pairs in corpus: (\"h\" \"ug\", 15), (\"p\" \"ug\", 5), (\"p\" \"u\", 5), (\"u\" \"n\", 16), (\"b\" \"u\", 4), (\"ug\" \"s\", 5)\n",
        "   Most frequent pair: (\"u\", \"n\")\n",
        "   ```\n",
        "   The second merge rule learned by the tokenizer is `(\"u\", \"n\") -> \"un\"`, which means that `\"un\"` will be added to the vocabulary, and the pair should be merged in all the words of the corpus.\n",
        "   At the end of this stage, the vocabulary and corpus look like this:\n",
        "   ```\n",
        "   Vocabulary: [\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\", \"ug\", \"un\"]\n",
        "   Corpus: (\"h\" \"ug\", 10), (\"p\" \"ug\", 5), (\"p\" \"un\", 12), (\"b\" \"un\", 4), (\"h\" \"ug\" \"s\", 5)\n",
        "   ```\n",
        "\n",
        "   Let's look at one more merge step:\n",
        "   ```\n",
        "   Pairs in corpus: (\"h\" \"ug\", 15), (\"p\" \"un\", 5), (\"p\" \"un\", 12), (\"b\" \"u\", 4), (\"ug\" \"s\", 5)\n",
        "   Most frequent pair: (\"h\", \"ug\")\n",
        "   ```\n",
        "   The next merge rule learned by the tokenizer is `(\"h\", \"ug\") -> \"hug\"`, which means that `\"hug\"` will be added to the vocabulary, and the pair should be merged in all the words of the corpus.\n",
        "   At the end of this stage, the vocabulary and corpus look like this:\n",
        "   ```\n",
        "   Vocabulary: [\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\", \"ug\", \"un\", \"hug\"]\n",
        "   Corpus: (\"hug\", 10), (\"p\" \"ug\", 5), (\"p\" \"un\", 12), (\"b\" \"un\", 4), (\"hug\" \"s\", 5)\n",
        "   ```\n",
        "\n",
        "   Continue like this until we reach the desired vocabulary size.\n",
        "5. The final vocabulary contains **frequent subword units**, which allows encoding unseen words as sequences of subwords.\n",
        "\n",
        "\n",
        "Have a look at the [re-implementation of BPE](https://huggingface.co/learn/llm-course/chapter6/5#implementing-bpe)!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NB0d1ExWiOqq",
        "outputId": "6ef2ab37-be60-4efe-9918-a68d00b6c2f4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['At', 'Ġeight', 'Ġo', \"'\", 'clock', 'Ġon', 'ĠThursday', 'Ġmorning', 'ĠArthur', 'Ġdidn', \"'t\", 'Ġfeel', 'Ġvery', 'Ġgood', '.']\n",
            "['Don', 'ner', 'st', 'ag', 'Ġmor', 'g', 'ens', 'Ġum', 'Ġa', 'cht', 'ĠU', 'hr', 'Ġf', 'Ã¼', 'hl', 'te', 'Ġs', 'ich', 'ĠArthur', 'Ġn', 'icht', 'Ġso', 'Ġgut', '.']\n",
            "['>', 'Ġ', 'Ġ', 'Ġ<']\n",
            "['...', 'Ġ:', '-(', 'Ġ:(', 'Ġ', '!!!!!!!!', '!!', 'Ċ', 'd']\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
        "print(tokenizer.tokenize(sentence_en))\n",
        "print(tokenizer.tokenize(sentence_de))\n",
        "print(tokenizer.tokenize(spaces))\n",
        "print(tokenizer.tokenize(\"... :-( :( !!!!!!!!!!\\nd\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5DscJx3f00J"
      },
      "source": [
        "## WordPiece\n",
        "\n",
        "WordPiece used in quite a few Transformer models based on BERT, such as DistilBERT, MobileBERT, Funnel Transformers, and MPNET. \n",
        "It’s very similar to BPE in terms of the training, but the actual tokenization is done differently [[Reference](https://huggingface.co/learn/llm-course/chapter6/6)].\n",
        "\n",
        "Like BPE, WordPiece starts from a small vocabulary including the special tokens used by the model and the initial alphabet. \n",
        "It identifies subwords by adding a prefix (like ## for BERT), each word is initially split by adding that prefix to all the characters inside the word. \n",
        "So, for instance, \"pun\" gets split like this:\n",
        "```\n",
        "[\"p\", \"##u\", \"##n\"]\n",
        "```\n",
        "\n",
        "The main difference between BPE and WordPiece is in the way they select pairs to be merged during training.\n",
        "Instead of selecting the most frequent pair, WordPiece computes a score for each pair, using the following formula: \n",
        "$ score(pair) = \\frac{count(pair)}{count(first\\_subword) \\times count(second\\_subword)} $\n",
        "By dividing the frequency of the pair by the product of the frequencies of each of its parts, the algorithm prioritizes the merging of pairs where the individual parts are less frequent in the vocabulary. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Assume our corpus contains the words: `hug`, `pug`, `pun`, `bun`, `hugs` with the following frequencies:\n",
        "`(\"hug\", 10), (\"pug\", 5), (\"pun\", 12), (\"bun\", 4), (\"hugs\", 5)`.\n",
        "\n",
        "Which results in the following corpus representation at the beginning of the training:\n",
        "```\n",
        "(\"h\" \"##u\" \"##g\", 10), (\"p\" \"##u\" \"##g\", 5), (\"p\" \"##u\" \"##n\", 12), (\"b\" \"##u\" \"##n\", 4), (\"h\" \"##u\" \"##g\" \"##s\", 5)\n",
        "```\n",
        "and the initial vocabulary (without special tokens):\n",
        "```\n",
        "[\"b\", \"h\", \"p\", \"##g\", \"##n\", \"##s\", \"##u\"]\n",
        "```\n",
        "\n",
        "1. Count all symbol pairs and compute their scores.\n",
        "```\n",
        "   Pairs in corpus: (\"h\" \"##u\", 15), (\"##u\" \"##g\", 20), (\"p\" \"##u\", 5), (\"##u\" \"##n\", 16), (\"b\" \"##u\", 4), (\"##g\" \"##s\", 5)\n",
        "   Most frequent pair: (\"##u\", \"##g\"), but individual counts are high, resulting in a lower score of 1/36.\n",
        "   Highest scoring pair: (\"##g\", \"##s\") with a score of 1/20\n",
        "```\n",
        "2. Merge the highest scoring pair and replace all occurrences of the pair with a new symbol.\n",
        "   The first merge rule learned by the tokenizer is `(\"##g\", \"##s\") -> \"##gs\"`, which means that `\"##gs\"` will be added to the vocabulary, and the pair should be merged in all the words of the corpus.\n",
        "   At the end of this stage, the vocabulary and corpus look like this:\n",
        "```\n",
        "   Vocabulary: [\"b\", \"h\", \"p\", \"##g\", \"##n\", \"##s\", \"##u\", \"##gs\"]\n",
        "   Corpus: (\"h\" \"##u\" \"##g\", 10), (\"p\" \"##u\" \"##g\", 5), (\"p\" \"##u\" \"##n\", 12), (\"b\" \"##u\" \"##n\", 4), (\"h\" \"##u\" \"##gs\", 5)\n",
        "```\n",
        "3. Continue like this until we reach the desired vocabulary size.\n",
        "\n",
        "Have a look at the [re-implementation of WordPiece](https://huggingface.co/learn/llm-course/chapter6/6#implementing-wordpiece)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLp-xewwf2k5",
        "outputId": "77bfa96f-f35e-4a6d-ff21-95b7036b1d79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['at', 'eight', 'o', \"'\", 'clock', 'on', 'thursday', 'morning', 'arthur', 'didn', \"'\", 't', 'feel', 'very', 'good', '.']\n",
            "['don', '##ners', '##tag', 'mor', '##gens', 'um', 'ac', '##ht', 'uh', '##r', 'fu', '##hl', '##te', 'sic', '##h', 'arthur', 'nic', '##ht', 'so', 'gut', '.']\n",
            "['>', '<']\n",
            "['.', '.', '.', ':', '-', '(', ':', '(', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '.', 'a', '.']\n",
            "['wi', '##er', '##d']\n",
            "['weird']\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
        "print(tokenizer.tokenize(sentence_en))\n",
        "print(tokenizer.tokenize(sentence_de))\n",
        "print(tokenizer.tokenize(spaces))\n",
        "print(tokenizer.tokenize(\"... :-( :( !!!!!!!!!! .a.\"))\n",
        "print(tokenizer.tokenize(\"wierd\"))\n",
        "print(tokenizer.tokenize(\"weird\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZA_kPj4YhKaX"
      },
      "source": [
        "## Sentence Piece\n",
        "\n",
        "[SentencePiece](https://github.com/google/sentencepiece) is a re-implementation of sub-word units, an effective way to alleviate the open vocabulary problems in neural machine translation. \n",
        "SentencePiece supports two segmentation, byte-pair-encoding (BPE) [Sennrich et al.](https://aclanthology.org/P16-1162/) and unigram language model [Kudo.](https://arxiv.org/abs/1804.10959).\n",
        "SentencePiece is an unsupervised text tokenizer and detokenizer.\n",
        "It considers the text as a sequence of Unicode characters, and replaces spaces with a special character, `▁`. \n",
        "The other main feature of SentencePiece is reversible tokenization: \n",
        "since there is no special treatment of spaces, decoding the tokens is done simply by concatenating them and replacing the `_` with spaces — this results in the normalized text [[Reference](https://huggingface.co/learn/llm-course/en/chapter6/4#sentencepiece)].\n",
        "\n",
        "Find more details about SentencePiece in their [Github Repo README](https://github.com/google/sentencepiece?tab=readme-ov-file#overview)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CeICQLWchNMT",
        "outputId": "a81e47a4-7cbb-4bf7-aa65-65cb41ba7e76"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['▁At', '▁eight', '▁', 'o', \"'\", 'clock', '▁on', '▁Thursday', '▁morning', '▁Arthur', '▁didn', \"'\", 't', '▁feel', '▁very', '▁good', '.']\n",
            "['▁Don', 'ner', 's', 'tag', '▁mor', 'gen', 's', '▁um', '▁acht', '▁Uhr', '▁fühlt', 'e', '▁sich', '▁Arthur', '▁nicht', '▁so', '▁gut', '.']\n",
            "['▁>', '▁', '<']\n",
            "['▁', '...', '▁', ':', '-', '(', '▁', ':', '(', '▁', '!!!!!', '!!!!!', '▁', '.', 'a', '.']\n"
          ]
        }
      ],
      "source": [
        "from transformers import T5Tokenizer\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"google-t5/t5-base\")\n",
        "print(tokenizer.tokenize(sentence_en))\n",
        "print(tokenizer.tokenize(sentence_de))\n",
        "print(tokenizer.tokenize(spaces))\n",
        "print(tokenizer.tokenize(\"... :-( :( !!!!!!!!!! .a.\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKf-6H9bkQb2"
      },
      "source": [
        "## MorphBPE, MorphPiece and other Morpheme Tokenizations\n",
        "\n",
        "[MorphBPE](https://arxiv.org/abs/2502.00894) is a morphology-aware extension of BPE that integrates linguistic structure into subword tokenization while preserving statistical efficiency.\n",
        "\n",
        "[MorphPiece](https://arxiv.org/pdf/2307.07262v2) is based partly on morphological segmentation of the underlying text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SBP9pEwe8ly"
      },
      "source": [
        "# Tweet Tokenizer\n",
        "\n",
        "[Tweet Tokenizer](https://www.nltk.org/api/nltk.tokenize.casual.html#nltk.tokenize.casual.TweetTokenizer) in NLTK is a specialized tokenizer designed for tokenizing social media text, especially tweets. It handles the quirks of informal writing on platforms like Twitter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HcQdHDKe703",
        "outputId": "4038fed5-f73f-4491-ab3d-46ff24594192"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['This', 'is', 'a', 'cooool', '#', 'dummysmiley', ':', ':', '-', ')', ':', '-P', '<', '3', 'and', 'some', 'arrows', '<', '>', '-', '>', '<', '--', '@', 'tag']\n",
            "['This', 'is', 'a', 'cooool', '#dummysmiley', ':', ':-)', ':-P', '<3', 'and', 'some', 'arrows', '<', '>', '->', '<--', '@tag']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "tweet = \"This is a cooool #dummysmiley: :-) :-P <3 and some arrows < > -> <-- @tag\"\n",
        "print(nltk.word_tokenize(tweet))\n",
        "print(TweetTokenizer().tokenize(tweet))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "8SBP9pEwe8ly"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
