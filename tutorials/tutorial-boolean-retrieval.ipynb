{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTl3bOcLawgB"
      },
      "source": [
        "# Building a Boolean Search Engine from Scratch\n",
        "\n",
        "In this tutorial, we will build a simple boolean search engine from scratch in Python. We will learn how to ...\n",
        "- ... split text with a tokenizer\n",
        "- ... parse queries with operators\n",
        "- ... implement efficient operations on posting lists efficiently\n",
        "- ... store documents and posting lists in an index\n",
        "- ... search that index."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEk39hfHzCMa"
      },
      "source": [
        "Start by installing the required Python library dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7isKeuvii1Ue"
      },
      "outputs": [],
      "source": [
        "!pip install -q nltk lark more-itertools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEwrLN97jQ_f"
      },
      "source": [
        "## Preliminaries 1: Tokenizing Text\n",
        "\n",
        "Our index will later store _tokens_ of each document, to make it searchable.\n",
        "The _tokenizer_ is responsible for splitting a given text document into tokens (or words). Here, we use NLTK's `MWETokenizer` which you can call as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-mHaWW1Si3tO"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import MWETokenizer\n",
        "tokenizer = MWETokenizer()\n",
        "print(tokenizer.tokenize(\"The statue of liberty is the most famous statue in new york\".split()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IW7K6Xyczmrr"
      },
      "source": [
        "Currently, the tokenizer does not handle \"New York\" well. It should not split this term.\n",
        "As we use a multi-word tokenizer, we can add multi-word tokens through the `add_mwe` function. In the following example, this will result in `new york` being mapped to the token `new_york`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wBgAoLNGzmRT"
      },
      "outputs": [],
      "source": [
        "tokenizer.add_mwe((\"new\", \"york\"))\n",
        "tokenizer.add_mwe((\"statue\", \"of\", \"liberty\"))\n",
        "print(tokenizer.tokenize(\"The statue of liberty is the most famous statue in new york\".split()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsXRTM6bqdbg"
      },
      "source": [
        "## Preliminaries 2: The Query Parser\n",
        "\n",
        "A boolean query generally consists of a propositional logical formular where the variables are tokens. Formally, we can describe this as a context free grammar:\n",
        "```\n",
        "<expr> := TOKEN\n",
        "       | ( <expr> )\n",
        "       | <expr> AND <expr>\n",
        "       | <expr> OR <expr>\n",
        "       | <expr> AND NOT <expr>\n",
        "```\n",
        "where `TOKEN`, `AND`, `OR`, and `NOT` are terminals. Here, we already provide a parser for you which you can call using `parser.parse(query)` to get the operation tree for `query`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4gGHyAOqYDY"
      },
      "outputs": [],
      "source": [
        "from lark import Lark, Tree, Token, tree\n",
        "from rich import print as rprint\n",
        "\n",
        "parser = Lark('''\n",
        "            ?expression: andexpr | orexpr | andnotexpr | \"(\" expression \")\" | WORD\n",
        "            andexpr:    expression \"AND\" expression\n",
        "            orexpr:     expression \"OR\" expression\n",
        "            andnotexpr:    expression \"AND NOT\" expression\n",
        "\n",
        "            %import common.WORD   // imports from terminal library\n",
        "            %ignore \" \"           // Disregard spaces in text\n",
        "         ''', start=\"expression\", )\n",
        "\n",
        "example = parser.parse(\"hello AND world OR (foo AND NOT x) AND bar\")\n",
        "print(example)\n",
        "rprint(example)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JxOqyet4dPV"
      },
      "source": [
        "## Efficient Operations on Posting Lists\n",
        "\n",
        "### Generators and Iterators\n",
        "When dealing with large datasets or streams of data, loading everything into memory at once can be inefficient or even impossible. Generators in Python provide an elegant solution to this problem. They allow you to generate items one at a time, on demand. This not only conserves memory but also enables you to process data as it becomes available, making generators invaluable for tasks like reading large files, streaming data, implementing infinite sequences, or (in our case) handling large lists.\n",
        "\n",
        "You can create a generator using the `yield` keyword. For example:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njkXqBwW8uQ2"
      },
      "outputs": [],
      "source": [
        "def count_up_to(n: int):\n",
        "    count = 1\n",
        "    while count <= n:\n",
        "        yield count\n",
        "        count += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCjbnRMQ8uuj"
      },
      "source": [
        "This generator produces numbers one at a time, which you can iterate over in a loop. However, generators do not support random access since elements are not stored. This means that you cannot access a generator's items by their position like you could with a list. Instead, generators can only be *iterated* over in a `for` loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUfA2hGi84_0"
      },
      "outputs": [],
      "source": [
        "for x in count_up_to(3):\n",
        "  print(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NocMrwU85QG"
      },
      "source": [
        "\n",
        "Sometimes we whish to *peek* an iterator, i.e., look at its first value without stepping the iterator. Iterators themselves don't implement this functionality but the `peekable` utility from the `more_itertools` library lets you wrap a generator and peek at the next value without consuming it. For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wuop2k5o88la"
      },
      "outputs": [],
      "source": [
        "from more_itertools import peekable\n",
        "\n",
        "gen = peekable(count_up_to(5))\n",
        "print(gen.peek())  # Outputs: 1\n",
        "print(next(gen))   # Outputs: 1\n",
        "print(next(gen))   # Outputs: 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuo4sHvBUsU7"
      },
      "source": [
        "A peekable generator also has a handy check if the iterator is empty, by casting it to a boolean."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BvIIceJGU5Wh"
      },
      "outputs": [],
      "source": [
        "from more_itertools import peekable\n",
        "\n",
        "gen = peekable(count_up_to(3))\n",
        "if gen:\n",
        "  print(\"Not empty.\")\n",
        "else:\n",
        "  print(\"Empty\")\n",
        "\n",
        "# Now, consume all elements of the generator.\n",
        "for x in gen:\n",
        "  print(x)\n",
        "\n",
        "if gen:\n",
        "  print(\"Not empty.\")\n",
        "else:\n",
        "  print(\"Empty\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yG5cgv8F-pHM"
      },
      "source": [
        "This makes it possible to inspect upcoming values while still benefiting from the memory efficiency of generators."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4iCaWTR8-Bb"
      },
      "source": [
        "### Efficient, Union, Intersect, and Set Difference\n",
        "\n",
        "With the recap about generators and iterators out of the way, we can use them to efficiently implement the intersection, union, and set-difference of our posting lists:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DFhKKcAV4hjy"
      },
      "outputs": [],
      "source": [
        "from typing import Iterable\n",
        "from more_itertools import peekable\n",
        "\n",
        "\n",
        "def intersect_postings(list1: Iterable[int], list2: Iterable[int]) -> Iterable[int]:\n",
        "  \"\"\"\n",
        "  This function takes two **sorted** posting lists and efficiently intersects them.\n",
        "  Use generators to iterate the intersection and to avoid as much copying as possible.\n",
        "  \"\"\"\n",
        "  raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJb47uYXW1oF"
      },
      "outputs": [],
      "source": [
        "def union_postings(list1: Iterable[int], list2: Iterable[int]) -> Iterable[int]:\n",
        "  \"\"\"\n",
        "  This function takes two **sorted** posting lists and efficiently forms their union.\n",
        "  Use generators to iterate the intersection and to avoid as much copying as possible.\n",
        "  \"\"\"\n",
        "  raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgCxF-hyW1Pv"
      },
      "outputs": [],
      "source": [
        "def setminus_postings(list1: Iterable[int], list2: Iterable[int]) -> Iterable[int]:\n",
        "  \"\"\"\n",
        "  This function takes two **sorted** posting lists and efficiently calculates\n",
        "  their set difference, i.e. returns all elements in list1 that are not in list2.\n",
        "  Use generators to iterate the intersection and to avoid as much copying as possible.\n",
        "  \"\"\"\n",
        "  raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wx1c_2Rg3g8m"
      },
      "source": [
        "### Task 1: Efficient Posting List Operations\n",
        "\n",
        "Implement `intersect_postings()` and `union_postings()` above efficiently by exploiting the ordered nature of posting lists. You can use the following code to check your implementation. (Leave `setminus_postings()` unmodified for now.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D6QIYoLN3Lut"
      },
      "outputs": [],
      "source": [
        "print(list(intersect_postings([1, 2, 6, 9, 10, 12, 13], [1, 2, 4, 8])))\n",
        "print(list(union_postings([1, 2, 6, 9, 10, 12, 13], [1, 2, 4, 8])))\n",
        "print(list(setminus_postings([1, 2, 6, 9, 10, 12, 13], [1, 2, 4, 12])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpFG3vR1c0PR"
      },
      "source": [
        "You can check if your implementation is correct by running the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PP_2PwAIX0VC"
      },
      "outputs": [],
      "source": [
        "if list(intersect_postings([1, 2, 6, 9, 10, 12, 13], [1, 2, 4, 8])) == [1, 2]:\n",
        "  print(\"Bravo! The intersection of postings works\")\n",
        "else:\n",
        "  print(\"Oops, that did not work.\")\n",
        "if list(union_postings([1, 2, 6, 9, 10, 12, 13], [1, 2, 4, 8])) == [1, 2, 4, 6, 8, 9, 10, 12, 13]:\n",
        "  print(\"Superb! The union of postings works\")\n",
        "else:\n",
        "  print(\"Oops, that did not work.\")\n",
        "if list(setminus_postings([1, 2, 6, 9, 10, 12, 13], [1, 2, 4, 12])) == [6, 9, 10, 13]:\n",
        "  print(\"Nice! The set difference of postings works\")\n",
        "else:\n",
        "  print(\"Oops, that did not work.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E89KTrGMj-o1"
      },
      "source": [
        "## The Index Structure\n",
        "\n",
        "Now that we have implemented all the necessary \"operators\" for the posting lists, and have tried out tokenization and query parsing, let's build an index, so that we can store and search documents. But do not worry! We will start step-by-step.\n",
        "\n",
        "First, just try to understand the dummy `Index` class.\n",
        "It will have a reference to the tokenizer (i.e., `self._tokenizer`), store the raw documents (i.e., in the `self._documents` attribute), and store the posting lists (i,e., `self._postings`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DNCdWzEDj86H"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import MWETokenizer\n",
        "\n",
        "\n",
        "class Index():\n",
        "  _tokenizer: MWETokenizer()\n",
        "  _documents: list[str]\n",
        "  _postings: dict[str, list[int]]\n",
        "\n",
        "  def __init__(self):\n",
        "    \"\"\"\n",
        "    Initialize a new, empty index.\n",
        "    \"\"\"\n",
        "    self._tokenizer = MWETokenizer()\n",
        "    self._documents = NotImplemented\n",
        "    self._postings = NotImplemented\n",
        "    raise NotImplementedError()\n",
        "\n",
        "  def _tokenize(self, text: str) -> set[str]:\n",
        "    \"\"\"\n",
        "    Split the given text into its tokens.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError()\n",
        "\n",
        "  def add_document(self, text: str):\n",
        "    \"\"\"\n",
        "    Add the new document to the document store and the index.\n",
        "    The raw document is added to the document store,\n",
        "    and add a pointer (i.e., the document ID) to the appropriate posting lists.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError()\n",
        "\n",
        "  def _search(self, query_tree: Tree | Token) -> Iterable[int]:\n",
        "    \"\"\"\n",
        "    Search for a pre-parsed query, given as a syntax tree,\n",
        "    and return the documents IDs of all documents matching the query.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError()\n",
        "\n",
        "  def search(self, query: str) -> list[str]:\n",
        "    \"\"\"\n",
        "    Search for a boolean text query and return all documents that\n",
        "    Takes a query in form of a string, parses it as a boolean query and returns\n",
        "    the content of all documents that fit the query.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError()\n",
        "\n",
        "  def get_documents(self) -> list[str]:\n",
        "    return self._documents\n",
        "\n",
        "  def get_postings(self) -> dict[str, list[int]]:\n",
        "    return self._postings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ie1fQNdo74zU"
      },
      "source": [
        "### Task 2: Indexing\n",
        "\n",
        "Implement the `__init__()` and `add_document()` methods for `Index` as described in their doc strings. For example, think about a reasonable way to initialize the lookup of posting lists by tokens (i.e., `self._postings`).\n",
        "You are free to choose the order of the documents in the posting list.\n",
        "\n",
        "*Hint:* The posting list does not have to be a `dict` as long as it is compatible with Python dictionaries. E.g., `defaultdict` may be a better choice here.\n",
        "\n",
        "After indexing, you can check the posting lists with the `get_postings()` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UP7hILEP75ED"
      },
      "outputs": [],
      "source": [
        "docs = [\n",
        "    \"the statue of liberty is the most famous statue in new york built gustave eiffel\",\n",
        "    \"where is new york\",\n",
        "    \"who built the empire state building\"\n",
        "]\n",
        "\n",
        "index = Index()\n",
        "\n",
        "for doc in docs:\n",
        "  index.add_document(doc)\n",
        "\n",
        "print(index.get_postings())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbeU95Cmjlm9"
      },
      "source": [
        "Let us quickly verify if the indexing has worked."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_WJvn79vjNQm"
      },
      "outputs": [],
      "source": [
        "postings = index.get_postings()\n",
        "if \"eiffel\" in postings and postings[\"eiffel\"] == [0]:\n",
        "  print(\"Congratulations! The indexing works.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQ3e7hhblDYA"
      },
      "source": [
        "### Task 3: Search\n",
        "\n",
        "Now, we would like to search the index by entering some query.\n",
        "You have probably noticed two search functions in the template code: `_search()` and `search()` (without underscore).\n",
        "The latter, `search()`, is what you would want as a user: Given a text query, return text documents.\n",
        "The `_search()` is a simplification where we already have a parsed query tree and just return the matching document IDs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rPCICU43Xhu"
      },
      "source": [
        "#### Task 3a\n",
        "\n",
        "Let us first implement the easier `_search()` method as described in its doc string.\n",
        "\n",
        "_Hints:_\n",
        "You can check if the query is a leaf (i.e., has no sub-trees) using the builtin `isinstance()` function (for example: `isinstance(\"test\", str) == True`). A leaf's text is stored in its `leaf.value` attribute. A trees left and right children are accessed like so `left, right = tree.children`. And the operator type is in the `tree.data` attribute.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxIU5tBw3XyM"
      },
      "outputs": [],
      "source": [
        "tree = parser.parse(\"gustave AND eiffel\")\n",
        "print(list(index._search(tree)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGY8_p5Gkbry"
      },
      "source": [
        "#### Task 3b:\n",
        "\n",
        "You can now implement the public `search()` function that directly takes a string as the query and returns the documents (as opposed to just the IDs).\n",
        "\n",
        "_Hint:_ Try to re-use your `_search()` function here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3JCQUk4sTpI"
      },
      "outputs": [],
      "source": [
        "print(index.search(\"gustave AND eiffel\"))\n",
        "print(index.search(\"(new AND york) OR built\"))\n",
        "print(index.search(\"new AND (york OR built)\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wfu8rBLD3N8E"
      },
      "source": [
        "### Task 4: The AND NOT Operator\n",
        "\n",
        "Modify the code above to implement `AND NOT`. Why do you think, don't we implement a general `NOT` operator?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sr4uDCGX3MK1"
      },
      "outputs": [],
      "source": [
        "print(index.search(\"((new AND york) OR built) AND NOT liberty\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nz0kMa2869iO"
      },
      "source": [
        "### Task 5: Multi-Word Tokenization\n",
        "\n",
        "Update the indexer to tokenize `new york` and `statue of liberty` as a single token each. What would a query requesting information on the Statue of Liberty in New York now look like?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Npp4Q__YOr3d"
      },
      "outputs": [],
      "source": [
        "# TODO: formulate a query requesting information about new york and the statue of liberty\n",
        "print(index.search(\"\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfMvKqGwNitB"
      },
      "source": [
        "### Task 6 (Bonus): Suffix Wildcards\n",
        "\n",
        "Update the indexer to support suffix wildcards.\n",
        "\n",
        "The suffix wildcard, `*`, matches any sequence of characters (even the empty sequence). For example, the pattern `he*` should match, among others, the tokens `he`, `hello`, and `hey`. What part of the indexer have to be updated? What data structure is best suited to find matching patterns?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GbbuAir-NiLO"
      },
      "outputs": [],
      "source": [
        "print(index.search(\"wh*\"))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
